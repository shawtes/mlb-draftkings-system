"""
Optimized Training Script for HP Omen 35L

This script optimizes the training process for the HP Omen 35L system by:
1. Reducing memory usage and computational overhead
2. Optimizing hyperparameter search iterations
3. Implementing efficient model stacking
4. Using GPU acceleration when available
5. Implementing robust error handling and fallback mechanisms

Key optimizations:
- Reduced hyperparameter search iterations (20-25 instead of 100+)
- Simplified model architecture to avoid excessive parallelization
- Chunked data processing optimized for 16GB RAM
- GPU-accelerated XGBoost with proper device handling
- Sequential processing for stability
"""

import pandas as pd
import numpy as np
import time
import warnings
import joblib
import os
import sys
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import torch

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

def optimize_training_for_omen_35l():
    """
    Optimized training configuration for HP Omen 35L
    """
    print("üöÄ HP Omen 35L Training Optimization")
    print("=" * 50)
    
    # System detection
    import psutil
    cpu_count = psutil.cpu_count(logical=True)
    memory_gb = psutil.virtual_memory().total / (1024**3)
    gpu_available = torch.cuda.is_available()
    
    print(f"CPU Cores: {cpu_count}")
    print(f"RAM: {memory_gb:.1f} GB")
    print(f"GPU Available: {gpu_available}")
    
    # Optimized settings
    config = {
        'chunk_size': 25000 if memory_gb >= 16 else 15000,
        'hyperparameter_iterations': 25 if memory_gb >= 16 else 15,
        'n_jobs': min(4, cpu_count - 2),  # Leave cores for system
        'max_workers': 4,
        'xgb_n_estimators': 100 if gpu_available else 50,
        'rf_n_estimators': 100 if memory_gb >= 16 else 50,
        'cv_folds': 3,
        'use_gpu': gpu_available
    }
    
    print("\nOptimized Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print("=" * 50)
    
    return config

def create_optimized_models(config):
    """
    Create optimized models for HP Omen 35L
    """
    print("Creating optimized model architecture...")
    
    # GPU/CPU configuration for XGBoost
    if config['use_gpu']:
        xgb_params = {
            'tree_method': 'hist',
            'device': 'cuda',
            'objective': 'reg:squarederror',
            'n_jobs': 1,  # Let GPU handle parallelization
            'random_state': 42,
            'n_estimators': config['xgb_n_estimators'],
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8
        }
    else:
        xgb_params = {
            'tree_method': 'hist',
            'device': 'cpu',
            'objective': 'reg:squarederror',
            'n_jobs': config['n_jobs'],
            'random_state': 42,
            'n_estimators': config['xgb_n_estimators'],
            'max_depth': 6,
            'learning_rate': 0.1,
            'subsample': 0.8
        }
    
    # Base models with optimized settings
    base_models = [
        ('ridge', Ridge(alpha=1.0)),
        ('lasso', Lasso(alpha=1.0)),
        ('rf', RandomForestRegressor(
            n_estimators=config['rf_n_estimators'],
            max_depth=8,
            n_jobs=config['n_jobs'],
            random_state=42
        )),
        ('xgb', XGBRegressor(**xgb_params))
    ]
    
    # Stacking model with optimized final estimator
    stacking_model = StackingRegressor(
        estimators=base_models,
        final_estimator=XGBRegressor(
            **{**xgb_params, 'n_estimators': 75}  # Slightly fewer for final estimator
        ),
        n_jobs=1,  # Sequential to avoid conflicts
        cv=config['cv_folds']
    )
    
    # Hyperparameter search space (reduced for efficiency)
    param_grid = {
        'final_estimator__n_estimators': [50, 75, 100],
        'final_estimator__max_depth': [4, 6, 8],
        'final_estimator__learning_rate': [0.1, 0.15, 0.2]
    }
    
    return stacking_model, param_grid

def run_optimized_hyperparameter_search(stacking_model, param_grid, X, y, config):
    """
    Run optimized hyperparameter search
    """
    print("üîç Starting optimized hyperparameter search...")
    print(f"Testing {config['hyperparameter_iterations']} parameter combinations")
    
    search = RandomizedSearchCV(
        stacking_model,
        param_grid,
        n_iter=config['hyperparameter_iterations'],
        cv=config['cv_folds'],
        scoring='neg_mean_squared_error',
        n_jobs=1,  # Sequential for stability
        verbose=1,
        random_state=42
    )
    
    start_time = time.time()
    search.fit(X, y)
    elapsed_time = time.time() - start_time
    
    print(f"‚úÖ Hyperparameter search completed in {elapsed_time:.1f} seconds")
    print(f"Best CV Score: {-search.best_score_:.4f}")
    print("Best Parameters:")
    for param, value in search.best_params_.items():
        print(f"  {param}: {value}")
    
    return search.best_estimator_

def train_with_fallback(X, y, config):
    """
    Train model with robust fallback mechanisms
    """
    print("üöÄ Starting robust model training...")
    
    try:
        # Step 1: Create optimized models
        stacking_model, param_grid = create_optimized_models(config)
        
        # Step 2: Run hyperparameter search
        best_model = run_optimized_hyperparameter_search(
            stacking_model, param_grid, X, y, config
        )
        
        print("‚úÖ Advanced stacking model trained successfully!")
        return best_model, "advanced_stacking"
        
    except Exception as e:
        print(f"‚ùå Advanced training failed: {e}")
        print("üîÑ Falling back to simpler model...")
        
        try:
            # Fallback 1: Simple stacking without hyperparameter tuning
            simple_stacking, _ = create_optimized_models(config)
            simple_stacking.fit(X, y)
            print("‚úÖ Simple stacking model trained successfully!")
            return simple_stacking, "simple_stacking"
            
        except Exception as e2:
            print(f"‚ùå Simple stacking failed: {e2}")
            print("üîÑ Falling back to single XGBoost model...")
            
            try:
                # Fallback 2: Single XGBoost model
                xgb_params = {
                    'tree_method': 'hist',
                    'device': 'cuda' if config['use_gpu'] else 'cpu',
                    'objective': 'reg:squarederror',
                    'n_jobs': 1 if config['use_gpu'] else config['n_jobs'],
                    'random_state': 42,
                    'n_estimators': 100,
                    'max_depth': 6,
                    'learning_rate': 0.1
                }
                
                xgb_model = XGBRegressor(**xgb_params)
                xgb_model.fit(X, y)
                print("‚úÖ XGBoost fallback model trained successfully!")
                return xgb_model, "xgboost_fallback"
                
            except Exception as e3:
                print(f"‚ùå XGBoost fallback failed: {e3}")
                print("üîÑ Using Random Forest as final fallback...")
                
                # Final fallback: Random Forest
                rf_model = RandomForestRegressor(
                    n_estimators=50,
                    max_depth=8,
                    n_jobs=config['n_jobs'],
                    random_state=42
                )
                rf_model.fit(X, y)
                print("‚úÖ Random Forest final fallback completed!")
                return rf_model, "random_forest_fallback"

def save_optimized_model(model, model_type, config):
    """
    Save the trained model with optimization metadata
    """
    print(f"üíæ Saving {model_type} model...")
    
    # Create model metadata
    metadata = {
        'model_type': model_type,
        'training_config': config,
        'training_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'gpu_used': config['use_gpu']
    }
    
    # Save model
    model_path = 'c:/Users/smtes/OneDrive/Documents/draftkings project/MLB_DRAFTKINGS_SYSTEM/3_MODELS/optimized_model.joblib'
    joblib.dump(model, model_path)
    
    # Save metadata
    metadata_path = 'c:/Users/smtes/OneDrive/Documents/draftkings project/MLB_DRAFTKINGS_SYSTEM/3_MODELS/model_metadata.joblib'
    joblib.dump(metadata, metadata_path)
    
    print(f"‚úÖ Model saved to: {model_path}")
    print(f"‚úÖ Metadata saved to: {metadata_path}")

def main():
    """
    Main training function optimized for HP Omen 35L
    """
    print("üöÄ HP Omen 35L MLB Fantasy Points Training")
    print("=" * 60)
    
    start_time = time.time()
    
    # Step 1: Get optimized configuration
    config = optimize_training_for_omen_35l()
    
    # Step 2: Check if we have preprocessed data
    processed_data_path = 'c:/Users/smtes/OneDrive/Documents/draftkings project/MLB_DRAFTKINGS_SYSTEM/3_MODELS/processed_training_data.joblib'
    
    if os.path.exists(processed_data_path):
        print("üìÇ Loading preprocessed training data...")
        data = joblib.load(processed_data_path)
        X = data['X']
        y = data['y']
        print(f"‚úÖ Loaded {X.shape[0]} training samples with {X.shape[1]} features")
    else:
        print("‚ùå No preprocessed data found!")
        print("Please run the main training.py script first to generate processed data.")
        return
    
    # Step 3: Train model with fallback mechanisms
    model, model_type = train_with_fallback(X, y, config)
    
    # Step 4: Save the optimized model
    save_optimized_model(model, model_type, config)
    
    # Step 5: Performance summary
    elapsed_time = time.time() - start_time
    print("\n" + "=" * 60)
    print("üéØ HP Omen 35L Training Complete!")
    print(f"‚è±Ô∏è  Total Time: {elapsed_time:.1f} seconds")
    print(f"ü§ñ Model Type: {model_type}")
    print(f"üîß Configuration: {config['hyperparameter_iterations']} hyperparameter iterations")
    print(f"üíæ GPU Used: {'Yes' if config['use_gpu'] else 'No'}")
    print("=" * 60)

if __name__ == "__main__":
    main()
